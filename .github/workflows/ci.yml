name: CI Checks

# This workflow handles the complete CI pipeline for the Scry API including:
# - Linting (via golangci-lint)
# - Standard Testing (with test_without_external_deps tag to avoid external API calls)
# - Building
# - Optional Gemini API Integration Testing (without the test_without_external_deps tag)
#
# ARTIFACT NAMING REQUIREMENTS:
# - GitHub Actions artifact names must NOT contain forward slashes (/)
# - Package paths in matrix.package (like "internal/api") must be sanitized by replacing
#   slashes with hyphens (like "internal-api") before using in artifact names
# - The sanitization is handled by using steps like "Create sanitized package name"
#
# IMPORTANT NOTES ABOUT GEMINI API INTEGRATION TESTS:
# 1. These tests require a valid Gemini API key stored as a GitHub Secret named GEMINI_API_KEY
# 2. The tests connect to the real Gemini API and may incur costs
# 3. Tests can be affected by API rate limits, service outages, or API changes
# 4. They are configured to run:
#    - When manually triggered via the GitHub Actions UI (workflow_dispatch)
#    - On a weekly schedule (Monday at 2:00 AM UTC)
# 5. Consider reviewing test results after API version updates or when investigating potential
#    API integration issues
#
# For local testing with the real Gemini API, see instructions in:
# /internal/platform/gemini/TEST_README.md

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]
  # Manual trigger for the Gemini integration tests
  workflow_dispatch:
    inputs:
      run-gemini-tests:
        description: 'Run Gemini API integration tests'
        type: boolean
        default: true
        required: true
  # Weekly schedule for Gemini integration tests
  schedule:
    # Run at 2:00 AM UTC every Monday
    - cron: '0 2 * * 1'

# Centralized Go version - update this single value to change version across all jobs
env:
  GO_VERSION: '1.24'

permissions:
  contents: read # Default permission

jobs:
  pre-flight:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: scry_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true
      - name: Install required tools and libraries for CGo
        run: |
          echo "Installing PostgreSQL client tools, C compiler, and development libraries for CGo..."
          sudo apt-get update
          sudo apt-get install -y postgresql-client gcc libpq-dev

          # Verify installations
          echo "Verifying installed tools and libraries:"
          echo "PostgreSQL client version:"
          psql --version
          echo "GCC version:"
          gcc --version
          echo "libpq availability:"
          pkg-config --libs libpq || echo "libpq available (pkg-config not showing details)"
          echo "Listing PostgreSQL dev files:"
          ls -la /usr/include/postgresql/ || echo "PostgreSQL headers not found in default location"

      - name: Build main application
        run: |
          echo "Running early build verification for main application..."
          echo "Running: go build ./cmd/server"
          go build ./cmd/server
          echo "Build verification completed successfully"

      - name: Run CI pre-flight checks
        run: |
          chmod +x ./scripts/ci-pre-flight.sh
          echo "CGO is enabled: CGO_ENABLED=$CGO_ENABLED"
          ./scripts/ci-pre-flight.sh
        env:
          # Enable CGo for PostgreSQL driver
          CGO_ENABLED: 1
          # Database connection variables
          DATABASE_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          SCRY_TEST_DB_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          SCRY_DATABASE_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          # Project root variable
          SCRY_PROJECT_ROOT: ${{ github.workspace }}
          # Other required environment variables
          SCRY_AUTH_JWT_SECRET: ci-test-jwt-secret-32-characters-long
          SCRY_AUTH_BCRYPT_COST: "10"
          SCRY_AUTH_TOKEN_LIFETIME_MINUTES: "60"
          SCRY_AUTH_REFRESH_TOKEN_LIFETIME_MINUTES: "10080"
          SCRY_LLM_GEMINI_API_KEY: ci-test-gemini-key
          SCRY_LLM_MODEL_NAME: gemini-2.0-flash
          SCRY_LLM_PROMPT_TEMPLATE_PATH: prompts/flashcard_template.txt
          SCRY_LLM_MAX_RETRIES: "3"
          SCRY_LLM_RETRY_DELAY_SECONDS: "2"
          SCRY_SERVER_PORT: "8080"
          SCRY_SERVER_LOG_LEVEL: info
          SCRY_TASK_WORKER_COUNT: "2"
          SCRY_TASK_QUEUE_SIZE: "100"
          SCRY_TASK_STUCK_TASK_AGE_MINUTES: "30"

  lint:
    name: Lint
    runs-on: ubuntu-latest
    needs: pre-flight
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }} # Using centralized version
          cache: true
      - name: Run golangci-lint
        uses: golangci/golangci-lint-action@v7
        with:
          version: v2.1.1 # Match pre-commit hook version
          args: --verbose --build-tags=integration,test_without_external_deps
        # Note: This includes gofmt and goimports checks, making a separate format job unnecessary

  test:
    name: Test - ${{ matrix.package }}
    runs-on: ubuntu-latest
    needs: pre-flight
    strategy:
      fail-fast: false # Allow other jobs to continue if one fails
      matrix:
        package:
          - cmd/server
          - internal/api
          - internal/api/middleware
          - internal/api/shared
          - internal/ciutil
          - internal/config
          - internal/domain
          - internal/domain/srs
          - internal/events
          - internal/generation
          - internal/platform/gemini
          - internal/platform/logger
          - internal/platform/postgres
          - internal/redact
          - internal/service
          - internal/service/auth
          - internal/service/card_review
          - internal/store
          - internal/task
          - internal/testutils
          - infrastructure
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: scry_test
        ports:
          - 5432:5432
        # Health check to ensure PostgreSQL is ready before running tests
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true
      - name: Install required tools and libraries for CGo
        run: |
          echo "Installing PostgreSQL client tools, C compiler, and development libraries for CGo..."
          sudo apt-get update
          sudo apt-get install -y postgresql-client jq gcc libpq-dev

          # Verify installations
          echo "Verifying installed tools and libraries:"
          echo "PostgreSQL client version:"
          psql --version
          echo "GCC version:"
          gcc --version
          echo "libpq availability:"
          pkg-config --libs libpq || echo "libpq available (pkg-config not showing details)"
          echo "Listing PostgreSQL dev files:"
          ls -la /usr/include/postgresql/ || echo "PostgreSQL headers not found in default location"

      - name: Build main application
        run: |
          echo "Building main application..."
          echo "Running: go build ./cmd/server"
          go build ./cmd/server
          echo "Build completed successfully"

      - name: Migration Smoke Test
        run: |
          echo "Running migration smoke test to verify database driver..."
          echo "Running: go run ./cmd/server -migrate=status"
          if go run ./cmd/server -migrate=status; then
            echo "Migration smoke test passed successfully"
          else
            echo "Migration smoke test failed - likely database driver issue"
            exit 1
          fi
        env:
          # Enable CGo for PostgreSQL driver
          CGO_ENABLED: 1
          # Database connection variables
          DATABASE_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          SCRY_TEST_DB_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          SCRY_DATABASE_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          # Project root variable
          SCRY_PROJECT_ROOT: ${{ github.workspace }}
          # Required environment variables
          SCRY_AUTH_JWT_SECRET: ci-test-jwt-secret-32-characters-long
          SCRY_LLM_GEMINI_API_KEY: ci-test-gemini-key
          SCRY_LLM_PROMPT_TEMPLATE_PATH: prompts/flashcard_template.txt

      - name: Verify database connectivity
        run: |
          echo "Verifying database connectivity with enhanced checks..."
          chmod +x ./scripts/wait-for-db.sh
          ./scripts/wait-for-db.sh --attempts 20 --sleep 3 --timeout 90
        env:
          DATABASE_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          SCRY_TEST_DB_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          SCRY_DATABASE_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable

      - name: Reset and prepare database
        run: |
          # Make the reset script executable
          chmod +x ./scripts/reset-test-db.sh

          # Reset the database to ensure clean state
          echo "Resetting database to ensure clean state..."
          ./scripts/reset-test-db.sh

          # Apply migrations to the clean database
          echo "Applying database migrations..."
          echo "Running: go run ./cmd/server -migrate=up"
          go run ./cmd/server -migrate=up

          # Execute detailed migration validation
          echo "Validating migrations were applied correctly..."
          echo "Running: go run ./cmd/server -validate-migrations -verbose"
          # This uses our enhanced validation logic that will fail CI if migrations aren't properly applied
          go run ./cmd/server -validate-migrations -verbose

          # Log migration table content for diagnostics
          echo "Migration table contents (diagnostic info):"
          PGPASSWORD=postgres psql -h localhost -p 5432 -U postgres -d scry_test -c "SELECT version_id, is_applied FROM schema_migrations ORDER BY version_id;"
        env:
          # Enable CGo for PostgreSQL driver
          CGO_ENABLED: 1
          # Database connection variables
          DATABASE_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          SCRY_TEST_DB_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          SCRY_DATABASE_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          # Project root variable (leveraging T036 fix)
          SCRY_PROJECT_ROOT: ${{ github.workspace }}

          # Required environment variables for configuration validation
          SCRY_AUTH_JWT_SECRET: ci-test-jwt-secret-32-characters-long
          SCRY_AUTH_BCRYPT_COST: "10"
          SCRY_AUTH_TOKEN_LIFETIME_MINUTES: "60"
          SCRY_AUTH_REFRESH_TOKEN_LIFETIME_MINUTES: "10080"
          SCRY_LLM_GEMINI_API_KEY: ci-test-gemini-key
          SCRY_LLM_MODEL_NAME: gemini-2.0-flash
          SCRY_LLM_PROMPT_TEMPLATE_PATH: prompts/flashcard_template.txt
          SCRY_LLM_MAX_RETRIES: "3"
          SCRY_LLM_RETRY_DELAY_SECONDS: "2"
          SCRY_SERVER_PORT: "8080"
          SCRY_SERVER_LOG_LEVEL: info
          SCRY_TASK_WORKER_COUNT: "2"
          SCRY_TASK_QUEUE_SIZE: "100"
          SCRY_TASK_STUCK_TASK_AGE_MINUTES: "30"
      - name: Run tests
        run: |
          # Reset database again right before tests to ensure clean state
          echo "Resetting database before tests..."
          ./scripts/reset-test-db.sh

          # Apply migrations again to ensure proper schema
          echo "Re-applying migrations for tests..."
          echo "Running: go run ./cmd/server -migrate=up"
          go run ./cmd/server -migrate=up

          # Validate migrations before running tests
          echo "Validating migrations before tests..."
          echo "Running: go run ./cmd/server -validate-migrations"
          go run ./cmd/server -validate-migrations

          echo "::group::Running tests for package: ${{ matrix.package }}"
          PACKAGE_NAME=$(echo "${{ matrix.package }}" | tr '/' '-')

          # Log environment info for better diagnostics
          echo "=== Test Environment Information ==="
          echo "Go version: $(go version)"
          echo "CGO enabled: CGO_ENABLED=$CGO_ENABLED"
          echo "Test directory: $(pwd)"
          echo "Package path: ${{ matrix.package }}"
          echo "Database URL: $DATABASE_URL"
          echo "=================================="

          # Run tests with enhanced logging
          echo "Running tests with enhanced verbosity and logging:"
          echo "Running: go test -v -json -race -coverprofile=coverage-${PACKAGE_NAME}.out -tags=integration,test_without_external_deps ./${{ matrix.package }}/... | tee test-results-${PACKAGE_NAME}.json"

          # Set GOTEST_DEBUG for additional debug output for failed tests
          GOTEST_DEBUG=1 GODEBUG=gctrace=0 go test -v -json -race -coverprofile=coverage-${PACKAGE_NAME}.out -tags=integration,test_without_external_deps ./${{ matrix.package }}/... | tee test-results-${PACKAGE_NAME}.json

          TEST_EXIT_CODE=${PIPESTATUS[0]}
          echo "Test exit code: $TEST_EXIT_CODE"
          echo "::endgroup::"

          # Extract and format test failures with enhanced details if any exist
          if [ -f test-results-${PACKAGE_NAME}.json ]; then
            # Count failed tests
            FAILURE_COUNT=$(grep -c '"Action":"fail"' test-results-${PACKAGE_NAME}.json || echo "0")

            if [ "$FAILURE_COUNT" -gt "0" ]; then
              echo "::error::Found $FAILURE_COUNT test failures in package ${{ matrix.package }}"
              echo "::group::Test Failure Details for ${{ matrix.package }}"
              echo "=== TEST FAILURE SUMMARY ==="

              # Get basic failure information
              echo "Failed tests:"
              grep '"Action":"fail"' test-results-${PACKAGE_NAME}.json | jq -r '"- \(.Package): \(.Test ?? "package failure")"' || true

              # Extract more detailed failure information including output and error messages
              echo -e "\nDetailed failure information:"
              grep -A 1 '"Action":"fail"' test-results-${PACKAGE_NAME}.json |
                grep -v '"Action":"fail"' |
                jq -r 'select(.Output != null) | "Test: \(.Test)\nOutput: \(.Output)\n---"' || true

              echo -e "\nError messages:"
              grep '"Action":"fail"' test-results-${PACKAGE_NAME}.json |
                jq -r 'select(.Output != null) | .Output' |
                grep -E "Error:|Failed|Failure|Panic:" || echo "No specific error messages found"

              echo "==========================="
              echo "::endgroup::"
            else
              echo "::notice::All tests passed in package ${{ matrix.package }}"
            fi

            # Store exit code for later check
            echo "$TEST_EXIT_CODE" > "test-exit-code-${PACKAGE_NAME}.txt"
          else
            echo "::warning::No test results file found for ${{ matrix.package }}"
          fi
        env:
          # Enable CGo for PostgreSQL driver
          CGO_ENABLED: 1
          # Database connection variables
          DATABASE_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          SCRY_TEST_DB_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          SCRY_DATABASE_URL: postgres://postgres:postgres@localhost:5432/scry_test?sslmode=disable
          # Project root variable (leveraging T036 fix)
          SCRY_PROJECT_ROOT: ${{ github.workspace }}

          # Required environment variables for configuration validation
          SCRY_AUTH_JWT_SECRET: ci-test-jwt-secret-32-characters-long
          SCRY_AUTH_BCRYPT_COST: "10"
          SCRY_AUTH_TOKEN_LIFETIME_MINUTES: "60"
          SCRY_AUTH_REFRESH_TOKEN_LIFETIME_MINUTES: "10080"
          SCRY_LLM_GEMINI_API_KEY: ci-test-gemini-key
          SCRY_LLM_MODEL_NAME: gemini-2.0-flash
          SCRY_LLM_PROMPT_TEMPLATE_PATH: prompts/flashcard_template.txt
          SCRY_LLM_MAX_RETRIES: "3"
          SCRY_LLM_RETRY_DELAY_SECONDS: "2"
          SCRY_SERVER_PORT: "8080"
          SCRY_SERVER_LOG_LEVEL: debug # Set to debug for more detailed logs
          GOLOG_LOG_LEVEL: debug
          GO_TEST_VERBOSE: "1"
          GO_TEST_LOG_LEVEL: debug
          SCRY_TASK_WORKER_COUNT: "2"
          SCRY_TASK_QUEUE_SIZE: "100"
          SCRY_TASK_STUCK_TASK_AGE_MINUTES: "30"
      - name: Check package coverage
        run: |
          PACKAGE_NAME=$(echo "${{ matrix.package }}" | tr '/' '-')
          echo "Running: go tool cover -func=coverage-${PACKAGE_NAME}.out"
          go tool cover -func=coverage-${PACKAGE_NAME}.out

          # Extract coverage percentage for this package
          total_coverage=$(go tool cover -func=coverage-${PACKAGE_NAME}.out | grep total: | awk '{print $3}' | sed 's/%//')
          echo "Coverage for ${{ matrix.package }}: $total_coverage"

          # Check if this package has a specific threshold in the config
          if [[ -f "coverage-thresholds.json" ]]; then
            # Check if this package should be excluded
            is_excluded=$(jq -r --arg pkg "${{ matrix.package }}" '.excluded_packages[] | select(. == $pkg) | length > 0' coverage-thresholds.json)
            if [[ "$is_excluded" == "true" ]]; then
              echo "Package ${{ matrix.package }} is excluded from coverage requirements"
              exit 0
            fi

            # Get package-specific threshold or use default
            threshold=$(jq -r --arg pkg "${{ matrix.package }}" '.package_thresholds[$pkg] // .default_threshold' coverage-thresholds.json)
            echo "Required coverage threshold for ${{ matrix.package }}: $threshold%"

            # Convert to integer (remove decimal part) for comparison
            coverage_int=$(echo $total_coverage | cut -d. -f1)
            if [[ $coverage_int -lt $threshold ]]; then
              echo "::error::Package ${{ matrix.package }} has coverage $total_coverage% which is below the required threshold of $threshold%"
              exit 1
            else
              echo "::notice::Package ${{ matrix.package }} meets coverage requirement: $total_coverage% (threshold: $threshold%)"
            fi
          else
            echo "::warning::coverage-thresholds.json not found, skipping package-specific threshold check"
          fi
      # Upload test artifacts with extended diagnostics
      # Create sanitized package name for artifact naming (replace / with -)
      - name: Create sanitized package name
        id: sanitize
        run: |
          # Replace forward slashes with hyphens in package name
          SANITIZED_PACKAGE=$(echo "${{ matrix.package }}" | tr '/' '-')
          echo "sanitized_package=$SANITIZED_PACKAGE" >> $GITHUB_OUTPUT
          echo "Sanitized package name: $SANITIZED_PACKAGE"

      - name: Upload test results and diagnostics
        uses: actions/upload-artifact@v4
        if: always() # Upload even if tests fail
        with:
          # Use sanitized package name to avoid GitHub Actions artifact naming restrictions
          name: test-results-${{ steps.sanitize.outputs.sanitized_package }}-${{ runner.os }}-${{ github.run_id }}
          path: |
            test-results-*.json
            coverage-*.out
            test-exit-code-*.txt
          retention-days: 30

      # For failed tests, gather and upload additional diagnostics
      - name: Gather diagnostics for failed tests
        if: failure() # Only run this when tests fail
        run: |
          echo "::group::Additional Diagnostics for ${{ matrix.package }}"

          # Get sanitized package name (same as in previous step)
          SANITIZED_PACKAGE=$(echo "${{ matrix.package }}" | tr '/' '-')

          # Create diagnostics directory with sanitized name
          mkdir -p "test-diagnostics-${SANITIZED_PACKAGE}"

          # Directory path with sanitized name for use in all commands below
          DIAG_DIR="test-diagnostics-${SANITIZED_PACKAGE}"

          # System information
          echo "System information:" > "${DIAG_DIR}/system-info.txt"
          uname -a >> "${DIAG_DIR}/system-info.txt"
          free -h >> "${DIAG_DIR}/system-info.txt" 2>/dev/null || echo "free command not available" >> "${DIAG_DIR}/system-info.txt"
          df -h >> "${DIAG_DIR}/system-info.txt" 2>/dev/null || echo "df command not available" >> "${DIAG_DIR}/system-info.txt"

          # Go environment
          echo "Go environment:" > "${DIAG_DIR}/go-env.txt"
          go env >> "${DIAG_DIR}/go-env.txt"
          go version >> "${DIAG_DIR}/go-env.txt"

          # Save test results JSON as parsed output for easier analysis
          PACKAGE_NAME=${SANITIZED_PACKAGE}
          if [ -f "test-results-${PACKAGE_NAME}.json" ]; then
            echo "Test failure details:" > "${DIAG_DIR}/test-failures.txt"
            # Extract and format test failures
            grep '"Action":"fail"' "test-results-${PACKAGE_NAME}.json" |
              jq -r 'select(.Test != null) | "- Test: \(.Test)\n  Package: \(.Package)\n  Output: \(.Output)\n---"' >> "${DIAG_DIR}/test-failures.txt" ||
              echo "No failure details available" >> "${DIAG_DIR}/test-failures.txt"

            # Save full test output for reference
            echo "Full test results JSON (for debugging):" > "${DIAG_DIR}/full-test-results.txt"
            cat "test-results-${PACKAGE_NAME}.json" >> "${DIAG_DIR}/full-test-results.txt"

            # Test statistics
            echo "Test statistics:" > "${DIAG_DIR}/test-stats.txt"
            PASSED_COUNT=$(grep -c '"Action":"pass"' "test-results-${PACKAGE_NAME}.json" || echo "0")
            FAILED_COUNT=$(grep -c '"Action":"fail"' "test-results-${PACKAGE_NAME}.json" || echo "0")
            SKIP_COUNT=$(grep -c '"Action":"skip"' "test-results-${PACKAGE_NAME}.json" || echo "0")
            echo "- Tests passed: $PASSED_COUNT" >> "${DIAG_DIR}/test-stats.txt"
            echo "- Tests failed: $FAILED_COUNT" >> "${DIAG_DIR}/test-stats.txt"
            echo "- Tests skipped: $SKIP_COUNT" >> "${DIAG_DIR}/test-stats.txt"
          else
            echo "No test results JSON file found" > "${DIAG_DIR}/test-info.txt"
          fi

          # Database information
          echo "Database information:" > "${DIAG_DIR}/db-info.txt"
          echo "Database URL: $DATABASE_URL" | sed 's/postgres:\/\/[^:]*:[^@]*@/postgres:\/\/user:pass@/' >> "${DIAG_DIR}/db-info.txt"
          PGPASSWORD=postgres psql -h localhost -p 5432 -U postgres -d scry_test -c "\\d" >> "${DIAG_DIR}/db-info.txt" 2>/dev/null || echo "Could not connect to database" >> "${DIAG_DIR}/db-info.txt"
          # Get PostgreSQL server info
          echo "PostgreSQL server information:" >> "${DIAG_DIR}/db-info.txt"
          PGPASSWORD=postgres psql -h localhost -p 5432 -U postgres -d scry_test -c "SELECT version();" >> "${DIAG_DIR}/db-info.txt" 2>/dev/null || echo "Could not get PostgreSQL version" >> "${DIAG_DIR}/db-info.txt"

          # Environment variables (redacted)
          echo "Environment variables (redacted):" > "${DIAG_DIR}/env-vars.txt"
          env | grep -v -E "SECRET|KEY|TOKEN|PASSWORD" | sort >> "${DIAG_DIR}/env-vars.txt"

          # Save package info
          echo "Package information:" > "${DIAG_DIR}/package-info.txt"
          echo "Package: ${{ matrix.package }}" >> "${DIAG_DIR}/package-info.txt"
          echo "Original package name: ${{ matrix.package }}" >> "${DIAG_DIR}/package-info.txt"
          echo "Sanitized package name: ${SANITIZED_PACKAGE}" >> "${DIAG_DIR}/package-info.txt"

          echo "::endgroup::"

          # Save sanitized package name for upload step
          echo "sanitized_package=${SANITIZED_PACKAGE}" >> $GITHUB_ENV

      # Upload diagnostics for failed tests
      - name: Upload failure diagnostics
        uses: actions/upload-artifact@v4
        if: failure() # Only upload when tests fail
        with:
          # Use sanitized package name from environment variable
          name: test-diagnostics-${{ env.sanitized_package }}-${{ runner.os }}-${{ github.run_id }}
          path: test-diagnostics-${{ env.sanitized_package }}
          retention-days: 30

      # Add job summary with artifact information
      - name: Create job summary with artifact information
        if: always() # Run this step regardless of test outcome
        run: |
          echo "## Test Summary for ${{ matrix.package }}" >> $GITHUB_STEP_SUMMARY

          # Add status badge
          if [ "${{ job.status }}" == "success" ]; then
            echo "✅ **Status**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Status**: Failed" >> $GITHUB_STEP_SUMMARY
          fi

          # Add artifact information
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "📊 [Test Results Artifact](${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID})" >> $GITHUB_STEP_SUMMARY

          # Add additional information for failed tests
          if [ "${{ job.status }}" != "success" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Diagnostic Information" >> $GITHUB_STEP_SUMMARY
            echo "📋 [Failure Diagnostics Artifact](${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID})" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "ℹ️ **To download and view artifacts:**" >> $GITHUB_STEP_SUMMARY
            echo "1. Go to the Actions tab in the repository" >> $GITHUB_STEP_SUMMARY
            echo "2. Select this workflow run" >> $GITHUB_STEP_SUMMARY
            echo "3. Scroll down to the Artifacts section" >> $GITHUB_STEP_SUMMARY
            echo "4. Download the relevant artifact zip file" >> $GITHUB_STEP_SUMMARY
          fi

  coverage-check:
    name: Overall Coverage Check
    runs-on: ubuntu-latest
    needs: test
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Install jq
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Download all coverage reports
        uses: actions/download-artifact@v4
        with:
          # This pattern will match all test result artifacts with sanitized names
          pattern: test-results-*
          path: coverage-reports/

      - name: Merge coverage reports
        run: |
          echo "Merging coverage reports from all packages..."
          # Create a temporary directory for all coverage files
          mkdir -p merged-coverage

          # Find all coverage files and copy them to the temp directory
          find coverage-reports -name "*.out" -exec cp {} merged-coverage/ \;

          # Use go tool to merge all coverage files
          echo "Coverage files found:"
          ls -la merged-coverage/

          # Create a merged coverage report
          echo "mode: set" > merged-coverage.out
          tail -q -n +2 merged-coverage/*.out >> merged-coverage.out || echo "No coverage files found to merge"

          # Display merged coverage
          echo "Merged coverage report:"
          go tool cover -func=merged-coverage.out

          # Check overall coverage threshold
          total_coverage=$(go tool cover -func=merged-coverage.out | grep total: | awk '{print $3}' | sed 's/%//')
          echo "Total project coverage: $total_coverage"

          # Get overall threshold from configuration
          if [[ -f "coverage-thresholds.json" ]]; then
            overall_threshold=$(jq -r '.default_threshold' coverage-thresholds.json)
            echo "Required overall coverage threshold: $overall_threshold%"
          else
            echo "::warning::coverage-thresholds.json not found, using default threshold of 70%"
            overall_threshold=70
          fi

          # Convert to integer (remove decimal part) for comparison
          coverage_int=$(echo $total_coverage | cut -d. -f1)
          if [ "$coverage_int" -lt "$overall_threshold" ]; then
            echo "::error::Overall code coverage is $total_coverage% which is below the required threshold of $overall_threshold%"
            exit 1
          else
            echo "::notice::Project meets overall coverage requirement: $total_coverage% (threshold: $overall_threshold%)"
          fi

      - name: Upload merged coverage
        uses: actions/upload-artifact@v4
        with:
          name: merged-coverage-${{ runner.os }}-${{ github.run_id }}
          path: merged-coverage.out
          retention-days: 30

  # Add a summary of artifacts for the entire workflow
  artifact-summary:
    name: Artifact Availability Summary
    runs-on: ubuntu-latest
    if: always() # Always run this job
    needs: [test, coverage-check, build]
    steps:
      - name: Summarize artifact availability
        run: |
          echo "## 📊 CI Artifacts Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The following artifacts are available for this workflow run:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Standard Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- Test results for each package" >> $GITHUB_STEP_SUMMARY
          echo "- Coverage reports" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### For Failed Tests" >> $GITHUB_STEP_SUMMARY
          echo "If tests failed, additional diagnostic information is available in:" >> $GITHUB_STEP_SUMMARY
          echo "- Test diagnostics artifacts (one per failed package)" >> $GITHUB_STEP_SUMMARY
          echo "- Full test logs with error details" >> $GITHUB_STEP_SUMMARY
          echo "- System and environment information" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### How to Access Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "1. Go to the **Actions** tab in the repository" >> $GITHUB_STEP_SUMMARY
          echo "2. Select this workflow run: [Run #${{ github.run_id }}](${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID})" >> $GITHUB_STEP_SUMMARY
          echo "3. Scroll down to the **Artifacts** section" >> $GITHUB_STEP_SUMMARY
          echo "4. Download the relevant artifact zip file" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add notice about artifacts
          echo "::notice::📋 Artifacts Summary Available: All test results and diagnostics for this workflow run can be viewed in the 'Artifact Availability Summary' job."

  build:
    name: Build
    runs-on: ubuntu-latest
    needs: pre-flight
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true
      - name: Build application
        run: |
          echo "Building application..."
          echo "Running: go build -v ./cmd/server/..."
          go build -v ./cmd/server/...
          echo "Build completed successfully"

  # Optional job that runs tests with the real Gemini API (without the test_without_external_deps build tag)
  # This job is triggered manually via workflow_dispatch or on a weekly schedule
  #
  # PURPOSE:
  # - Verify real-world integration with the Gemini API
  # - Catch breaking changes in the API that mocks might miss
  # - Ensure prompt templates work as expected with the actual API
  #
  # COSTS & CONSIDERATIONS:
  # - Each test run will make multiple API calls that may incur costs
  # - API calls may be subject to rate limits (adjust test timeout if needed)
  # - Tests might become flaky due to API availability or changes
  # - Weekly scheduled runs help catch issues proactively
  #
  # SECURITY:
  # - API key is stored as a GitHub Secret (GEMINI_API_KEY)
  # - The job will fail if the secret is not available
  # - No API key information should appear in logs
  test-gemini-integration:
    name: Test Gemini Integration
    # Run only when explicitly requested via workflow_dispatch or weekly schedule
    if: |
      github.event_name == 'workflow_dispatch' && inputs.run-gemini-tests ||
      github.event_name == 'schedule'
    runs-on: ubuntu-latest
    # Set a timeout to prevent excessive usage in case of API issues
    timeout-minutes: 15
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      # Check for required secret before continuing
      - name: Verify Gemini API key is available
        run: |
          if [ -z "${{ secrets.GEMINI_API_KEY }}" ]; then
            echo "::error::GEMINI_API_KEY secret is not configured."
            echo "::error::Please add the GEMINI_API_KEY secret in your repository settings."
            echo "::error::This is required for running tests with the real Gemini API."
            exit 1
          else
            echo "::notice::GEMINI_API_KEY secret is available. Proceeding with tests."
          fi

      # Configure any specific test environment variables
      - name: Configure test environment
        run: |
          echo "::notice::Preparing to run Gemini integration tests with actual API"
          echo "::notice::Tests will be run without the test_without_external_deps build tag"
          echo "::notice::Timeout set to 5 minutes to account for API latency and potential retries"

      # Install required libraries for CGo
      - name: Install required tools and libraries for CGo
        run: |
          echo "Installing C compiler and development libraries for CGo..."
          sudo apt-get update
          sudo apt-get install -y gcc libpq-dev

          # Verify installations
          echo "Verifying installed libraries:"
          echo "GCC version:"
          gcc --version
          echo "libpq availability:"
          pkg-config --libs libpq || echo "libpq available (pkg-config not showing details)"
          echo "Listing PostgreSQL dev files:"
          ls -la /usr/include/postgresql/ || echo "PostgreSQL headers not found in default location"

      # Run tests specifically for the Gemini package without the test_without_external_deps tag
      - name: Run Gemini integration tests
        id: tests
        run: |
          echo "::group::Running Gemini API integration tests"
          echo "Running tests against real Gemini API at $(date)"

          # Log environment info for better diagnostics
          echo "=== Test Environment Information ==="
          echo "Go version: $(go version)"
          echo "CGO enabled: CGO_ENABLED=$CGO_ENABLED"
          echo "Test directory: $(pwd)"
          echo "Test timeout: $GO_TEST_TIMEOUT"
          echo "=================================="

          echo "Running: go test -v -json -timeout 5m ./internal/platform/gemini/... | tee gemini-test-results.json"

          # Use enhanced logging variables
          GOTEST_DEBUG=1 GODEBUG=gctrace=0 go test -v -json -timeout 5m ./internal/platform/gemini/... | tee gemini-test-results.json

          TEST_EXIT_CODE=${PIPESTATUS[0]}
          echo "Test exit code: $TEST_EXIT_CODE"
          echo "::endgroup::"

          # Provide detailed error reporting
          if [ -f gemini-test-results.json ]; then
            # Count failed tests
            FAILURE_COUNT=$(grep -c '"Action":"fail"' gemini-test-results.json || echo "0")

            if [ "$FAILURE_COUNT" -gt "0" ]; then
              echo "::error::Found $FAILURE_COUNT test failures in Gemini API tests"
              echo "::group::Gemini API Test Failure Details"
              echo "=== TEST FAILURE SUMMARY ==="

              # Get basic failure information
              echo "Failed tests:"
              grep '"Action":"fail"' gemini-test-results.json | jq -r '"- \(.Package): \(.Test ?? "package failure")"' || true

              # Extract more detailed failure information including output and error messages
              echo -e "\nDetailed failure information:"
              grep -A 1 '"Action":"fail"' gemini-test-results.json |
                grep -v '"Action":"fail"' |
                jq -r 'select(.Output != null) | "Test: \(.Test)\nOutput: \(.Output)\n---"' || true

              echo -e "\nAPI Error messages:"
              grep '"Action":"fail"' gemini-test-results.json |
                jq -r 'select(.Output != null) | .Output' |
                grep -E "Error:|Failed|Failure|Panic:|API|Status Code:" || echo "No specific API error messages found"

              echo "==========================="
              echo "::endgroup::"
            else
              echo "::notice::All Gemini API tests passed successfully"
            fi
          else
            echo "::warning::No Gemini API test results file found"
          fi
        env:
          # Enable CGo for PostgreSQL driver
          CGO_ENABLED: 1
          # Enhanced logging for tests
          GO_TEST_VERBOSE: "1"
          GOTEST_DEBUG: "1"
          SCRY_SERVER_LOG_LEVEL: debug
          GOLOG_LOG_LEVEL: debug
          GO_TEST_LOG_LEVEL: debug
          # API key and timeout
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GO_TEST_TIMEOUT: 5m

      # Upload test results if any exist with enhanced diagnostics
      - name: Upload Gemini test results
        uses: actions/upload-artifact@v4
        if: always() # Upload even if tests fail
        with:
          name: gemini-test-results-${{ runner.os }}-${{ github.run_id }}
          path: |
            gemini-test-results.json
          retention-days: 30

      # For failed Gemini tests, gather and upload additional diagnostics
      - name: Gather diagnostics for failed Gemini tests
        if: failure() # Only run this when tests fail
        run: |
          echo "::group::Additional Diagnostics for Gemini API Tests"

          # Create diagnostics directory
          mkdir -p gemini-test-diagnostics

          # System information
          echo "System information:" > gemini-test-diagnostics/system-info.txt
          uname -a >> gemini-test-diagnostics/system-info.txt
          free -h >> gemini-test-diagnostics/system-info.txt 2>/dev/null || echo "free command not available" >> gemini-test-diagnostics/system-info.txt
          df -h >> gemini-test-diagnostics/system-info.txt 2>/dev/null || echo "df command not available" >> gemini-test-diagnostics/system-info.txt

          # Go environment
          echo "Go environment:" > gemini-test-diagnostics/go-env.txt
          go env >> gemini-test-diagnostics/go-env.txt
          go version >> gemini-test-diagnostics/go-env.txt

          # Save detailed test information
          if [ -f gemini-test-results.json ]; then
            # Test failure details
            echo "Test failure details:" > gemini-test-diagnostics/test-failures.txt
            # Extract and format test failures
            grep '"Action":"fail"' gemini-test-results.json |
              jq -r 'select(.Test != null) | "- Test: \(.Test)\n  Package: \(.Package)\n  Output: \(.Output)\n---"' >> gemini-test-diagnostics/test-failures.txt ||
              echo "No failure details available" >> gemini-test-diagnostics/test-failures.txt

            # Save full test output for reference
            echo "Full test results JSON (for debugging):" > gemini-test-diagnostics/full-test-results.txt
            cat gemini-test-results.json >> gemini-test-diagnostics/full-test-results.txt

            # Test statistics
            echo "Test statistics:" > gemini-test-diagnostics/test-stats.txt
            PASSED_COUNT=$(grep -c '"Action":"pass"' gemini-test-results.json || echo "0")
            FAILED_COUNT=$(grep -c '"Action":"fail"' gemini-test-results.json || echo "0")
            SKIP_COUNT=$(grep -c '"Action":"skip"' gemini-test-results.json || echo "0")
            echo "- Tests passed: $PASSED_COUNT" >> gemini-test-diagnostics/test-stats.txt
            echo "- Tests failed: $FAILED_COUNT" >> gemini-test-diagnostics/test-stats.txt
            echo "- Tests skipped: $SKIP_COUNT" >> gemini-test-diagnostics/test-stats.txt

            # Extract API errors to a separate file
            echo "API Errors:" > gemini-test-diagnostics/api-errors.txt
            grep '"Action":"fail"' gemini-test-results.json |
              jq -r 'select(.Output != null) | .Output' |
              grep -E "Error:|Failed|API|Status Code:|response body:" >> gemini-test-diagnostics/api-errors.txt ||
              echo "No specific API errors found" >> gemini-test-diagnostics/api-errors.txt

            # Copy test results file
            cp gemini-test-results.json gemini-test-diagnostics/
          else
            echo "No test results file found" > gemini-test-diagnostics/test-info.txt
          fi

          # Environment variables (redacted)
          echo "Environment variables (redacted):" > gemini-test-diagnostics/env-vars.txt
          env | grep -v -E "SECRET|KEY|TOKEN|PASSWORD" | sort >> gemini-test-diagnostics/env-vars.txt

          # Internet connectivity check
          echo "Internet connectivity check:" > gemini-test-diagnostics/connectivity.txt
          curl -s --head https://generativelanguage.googleapis.com/ | head -n 1 >> gemini-test-diagnostics/connectivity.txt 2>/dev/null || echo "Could not connect to Gemini API" >> gemini-test-diagnostics/connectivity.txt

          echo "::endgroup::"

      # Upload diagnostics for failed tests
      - name: Upload Gemini failure diagnostics
        uses: actions/upload-artifact@v4
        if: failure() # Only upload when tests fail
        with:
          # Using a safe name without forward slashes
          name: gemini-test-diagnostics-${{ runner.os }}-${{ github.run_id }}
          path: gemini-test-diagnostics
          retention-days: 30

      # Add a comprehensive summary of the test run with enhanced details
      - name: Gemini integration test summary
        if: always() # Run even if tests fail
        run: |
          echo "::group::Gemini API Integration Test Summary"

          # Create a summary report file
          SUMMARY_FILE="gemini-test-summary.txt"
          echo "=== GEMINI API INTEGRATION TEST SUMMARY ===" > $SUMMARY_FILE
          echo "Date: $(date)" >> $SUMMARY_FILE
          echo "Test result: ${{ steps.tests.outcome }}" >> $SUMMARY_FILE
          echo "" >> $SUMMARY_FILE

          if [ "${{ steps.tests.outcome }}" == "success" ]; then
            echo "::notice::✅ Gemini API integration tests completed successfully at $(date)"
            echo "Test Status: SUCCESS" >> $SUMMARY_FILE

            # Count passed tests
            if [ -f gemini-test-results.json ]; then
              PASSED_COUNT=$(grep -c '"Action":"pass"' gemini-test-results.json || echo "0")
              echo "Total tests passed: $PASSED_COUNT" >> $SUMMARY_FILE
            fi
          else
            echo "::warning::⚠️ Gemini API integration tests failed at $(date)"
            echo "::warning::This could be due to API changes, rate limits, or temporary service issues."
            echo "Test Status: FAILED" >> $SUMMARY_FILE
            echo "Possible causes:" >> $SUMMARY_FILE
            echo "- API changes" >> $SUMMARY_FILE
            echo "- Rate limits" >> $SUMMARY_FILE
            echo "- Temporary service issues" >> $SUMMARY_FILE
            echo "- Network connectivity problems" >> $SUMMARY_FILE
            echo "" >> $SUMMARY_FILE

            # Extract and format test failures if any exist
            if [ -f gemini-test-results.json ]; then
              echo "Failed tests:" >> $SUMMARY_FILE
              PASSED_COUNT=$(grep -c '"Action":"pass"' gemini-test-results.json || echo "0")
              FAILED_COUNT=$(grep -c '"Action":"fail"' gemini-test-results.json || echo "0")
              SKIP_COUNT=$(grep -c '"Action":"skip"' gemini-test-results.json || echo "0")

              echo "Test statistics:" >> $SUMMARY_FILE
              echo "- Passed: $PASSED_COUNT" >> $SUMMARY_FILE
              echo "- Failed: $FAILED_COUNT" >> $SUMMARY_FILE
              echo "- Skipped: $SKIP_COUNT" >> $SUMMARY_FILE
              echo "" >> $SUMMARY_FILE

              echo "Failed test details:" >> $SUMMARY_FILE
              grep '"Action":"fail"' gemini-test-results.json |
                jq -r '"- \(.Package): \(.Test ?? "package failure")"' >> $SUMMARY_FILE ||
                echo "Could not extract detailed failure information" >> $SUMMARY_FILE

              echo "" >> $SUMMARY_FILE
              echo "Error summary:" >> $SUMMARY_FILE
              grep '"Action":"fail"' gemini-test-results.json |
                jq -r 'select(.Output != null) | .Output' |
                grep -E "Error:|API error:|Status Code:|response body:" >> $SUMMARY_FILE ||
                echo "No specific API error details found" >> $SUMMARY_FILE
            fi
          fi

          echo "" >> $SUMMARY_FILE
          echo "IMPORTANT NOTES:" >> $SUMMARY_FILE
          echo "- These tests run with the actual Gemini API and may be affected by external factors" >> $SUMMARY_FILE
          echo "- Consider checking the Gemini API status if tests fail unexpectedly" >> $SUMMARY_FILE
          echo "- The failure diagnostics artifact contains additional information" >> $SUMMARY_FILE
          echo "====================================================" >> $SUMMARY_FILE

          # Output the summary to the log
          cat $SUMMARY_FILE

          # Save the summary file for artifacts
          mkdir -p gemini-test-diagnostics
          cp $SUMMARY_FILE gemini-test-diagnostics/

          echo "::endgroup::"

          # Always output these notices outside the group
          echo "::notice::These tests run with the actual Gemini API and may be affected by external factors."
          echo "::notice::Consider checking the Gemini API status if tests fail unexpectedly."

          # Add a prominent notice about diagnostics artifacts
          if [ "${{ steps.tests.outcome }}" != "success" ]; then
            echo "::notice::📊 Detailed diagnostics are available in the workflow artifacts."
          fi
